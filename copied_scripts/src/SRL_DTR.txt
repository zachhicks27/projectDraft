import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
from src.models import ActorNetwork, CriticNetwork  # Direct imports from your models

class MIMICDataset(Dataset):
    """Dataset for MIMIC data with temporal processing"""
    def __init__(self, data_path, split='train'):
        self.data_path = Path(data_path)
        self.split = split
        
        # Load split-specific data
        print(f"Loading {split} data...")
        self.demographics = pd.read_csv(self.data_path / f'{split}_demographics.csv')
        self.timeseries = pd.read_csv(self.data_path / f'{split}_timeseries.csv')
        self.diagnoses = pd.read_csv(self.data_path / f'{split}_diagnoses.csv')
        self.prescriptions = pd.read_csv(self.data_path / f'{split}_prescriptions.csv')
        
        # Print column names for debugging
        print("\nAvailable columns:")
        print("Timeseries columns:", self.timeseries.columns.tolist())
        print("Demographics columns:", self.demographics.columns.tolist())
        print("Diagnoses columns:", self.diagnoses.columns.tolist())
        print("Prescriptions columns:", self.prescriptions.columns.tolist())
        
        # Get unique admission IDs
        self.hadm_ids = self.demographics['hadm_id'].unique()
        print(f"Loaded {len(self.hadm_ids)} admissions for {split}")
        
        ## Convert categorical columns to numeric in demographics
        #for col in ['gender', 'religion', 'language', 'marital_status', 'ethnicity']:
        #    if col in self.demographics.columns:
        #        self.demographics[col] = pd.Categorical(self.demographics[col]).codes
        
        # Get unique admission IDs
        self.hadm_ids = self.demographics['hadm_id'].unique()
        print(f"Loaded {len(self.hadm_ids)} admissions for {split}")
        
        # Define feature columns (matching your config)
        self.lab_cols = ['dbp', 'fio2', 'GCS', 'blood_glucose', 'sbp', 'hr', 
                        'PH', 'rr', 'bos', 'temp', 'urine_output']
        self.demo_cols = ['age', 'gender', 'weight', 'height']
        
    def __len__(self):
        return len(self.hadm_ids)
    
    def __getitem__(self, idx):
        hadm_id = self.hadm_ids[idx]
        
        # Get data for specific admission
        demo = self.demographics[self.demographics['hadm_id'] == hadm_id][self.demo_cols].fillna(0).values[0]
        labs = self.timeseries[self.timeseries['hadm_id'] == hadm_id][self.lab_cols].fillna(0).values
        diag = self.diagnoses[self.diagnoses['hadm_id'] == hadm_id]['icd9_code'].fillna(0).values
        meds = self.prescriptions[self.prescriptions['hadm_id'] == hadm_id]['atc_code'].fillna(0).values
        
        # Ensure proper shapes
        if len(labs) == 0:
            labs = np.zeros((1, len(self.lab_cols)))
        if len(diag) == 0:
            diag = np.zeros(1)
        if len(meds) == 0:
            meds = np.zeros(1)
        
        # Convert to tensors
        return {
            'demographics': torch.FloatTensor(demo),
            'labs': torch.FloatTensor(labs),
            'diagnoses': torch.LongTensor(diag),
            'medications': torch.FloatTensor(meds),
            'hadm_id': hadm_id
        }

class ExperienceBuffer:
    """Experience replay buffer"""
    def __init__(self, max_size=10000):
        self.buffer = []
        self.max_size = max_size
    
    def add(self, state, action, reward, next_state, done):
        if len(self.buffer) >= self.max_size:
            self.buffer.pop(0)
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        indices = np.random.choice(len(self.buffer), batch_size, replace=False)
        states, actions, rewards, next_states, dones = zip(*[self.buffer[idx] for idx in indices])
        
        return (
            {k: torch.stack([s[k] for s in states]) for k in states[0].keys()},
            torch.stack(actions),
            torch.tensor(rewards, dtype=torch.float32),
            {k: torch.stack([s[k] for s in next_states]) for k in next_states[0].keys()},
            torch.tensor(dones, dtype=torch.float32)
        )

class SRL_DTR:
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")
        
        # Set random seeds
        torch.manual_seed(config.seed)
        np.random.seed(config.seed)
        
        # Initialize actor and critic networks
        self.actor = ActorNetwork(
            state_size=config.state_dim,
            action_size=config.med_size,
            batch_size=config.batch_size,
            tau=config.tau,
            learning_rate=config.lra,
            epsilon=config.epsilon,
            time_stamp=config.time_stamp,
            med_size=config.med_size,
            lab_size=config.lab_size,
            demo_size=config.demo_size,
            di_size=config.di_size,
            device=self.device
        ).to(self.device)
        
        self.critic = CriticNetwork(
            state_size=config.state_dim,
            action_size=config.med_size,
            batch_size=config.batch_size,
            tau=config.tau,
            learning_rate=config.lrc,
            epsilon=config.epsilon,
            time_stamp=config.time_stamp,
            med_size=config.med_size,
            lab_size=config.lab_size,
            demo_size=config.demo_size,
            di_size=config.di_size,
            action_dim=config.med_size,
            device=self.device
        ).to(self.device)
        
        # Initialize experience buffer
        self.buffer = ExperienceBuffer(max_size=10000)
        
        # Initialize optimizers
        self.actor_optimizer = torch.optim.Adam(
            self.actor.parameters(), 
            lr=config.lra
        )
        self.critic_optimizer = torch.optim.Adam(
            self.critic.parameters(), 
            lr=config.lrc
        )
        
        # Setup data loaders
        self.setup_data()
    
    def setup_data(self):
        self.train_dataset = MIMICDataset(self.config.processed_path, split='train')
        self.val_dataset = MIMICDataset(self.config.processed_path, split='val')
        
        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=0  # Set to 0 for easier debugging
        )
        
        self.val_loader = DataLoader(
            self.val_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=0  # Set to 0 for easier debugging
        )
    
    def calculate_reward(self, mortality):
        """Calculate reward based on mortality flag"""
        return torch.where(mortality == 1, 
                         torch.tensor(-15.0), 
                         torch.where(mortality == 0,
                                   torch.tensor(15.0),
                                   torch.tensor(0.0)))
    
    def train_step(self, batch):
        """Single training step"""
        # Move batch to device
        states = {k: v.to(self.device) for k, v in batch.items() 
                 if k not in ['hadm_id', 'medications']}
        doctor_actions = batch['medications'].to(self.device)
        
        # Forward pass
        actions = self.actor(
            states['labs'],
            states['diagnoses'],
            states['demographics']
        )
        q_values = self.critic(
            states['labs'],
            actions,
            states['diagnoses'],
            states['demographics']
        )
        
        # Calculate losses
        critic_loss = F.mse_loss(q_values, doctor_actions)
        
        # Combined actor loss (RL + SL)
        rl_loss = -q_values.mean()
        sl_loss = F.binary_cross_entropy(actions, doctor_actions)
        actor_loss = (1 - self.config.epsilon) * rl_loss + self.config.epsilon * sl_loss
        
        # Update networks
        self.critic_optimizer.zero_grad()
        critic_loss.backward(retain_graph=True)
        self.critic_optimizer.step()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # Update target networks
        self.actor.target_train()
        self.critic.target_train()
        
        return {
            'critic_loss': critic_loss.item(),
            'actor_loss': actor_loss.item(),
            'rl_loss': rl_loss.item(),
            'sl_loss': sl_loss.item()
        }
    
    def validate(self):
        """Validation step"""
        self.actor.eval()
        self.critic.eval()
        val_metrics = []
        
        with torch.no_grad():
            for batch in self.val_loader:
                states = {k: v.to(self.device) for k, v in batch.items() 
                         if k not in ['hadm_id', 'medications']}
                doctor_actions = batch['medications'].to(self.device)
                
                actions = self.actor(
                    states['labs'],
                    states['diagnoses'],
                    states['demographics']
                )
                
                # Calculate Jaccard similarity
                pred_binary = (actions > 0.5).float()
                intersection = (pred_binary * doctor_actions).sum(1)
                union = (pred_binary + doctor_actions).clamp(0, 1).sum(1)
                jaccard = (intersection / union.clamp(min=1e-8)).mean()
                
                val_metrics.append({
                    'jaccard': jaccard.item()
                })
        
        self.actor.train()
        self.critic.train()
        return {k: np.mean([m[k] for m in val_metrics]) for k in val_metrics[0]}
    
    def save_checkpoint(self, filename, epoch):
        """Save model checkpoint"""
        checkpoint_path = self.config.processed_path / 'checkpoints' / filename
        checkpoint_path.parent.mkdir(parents=True, exist_ok=True)
        
        torch.save({
            'epoch': epoch,
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'config': self.config
        }, checkpoint_path)
    
    def train(self):
        """Main training loop"""
        print(f"Starting training on device: {self.device}")
        best_jaccard = 0
        
        for epoch in range(self.config.episode_count):
            # Training
            self.actor.train()
            self.critic.train()
            epoch_losses = []
            
            for batch in tqdm(self.train_loader, desc=f"Epoch {epoch}"):
                losses = self.train_step(batch)
                epoch_losses.append(losses)
            
            # Calculate average losses
            avg_losses = {
                k: np.mean([loss[k] for loss in epoch_losses])
                for k in epoch_losses[0].keys()
            }
            
            # Validation
            if epoch % 10 == 0:
                val_metrics = self.validate()
                
                print(f"\nEpoch {epoch}")
                print("Training Losses:")
                for k, v in avg_losses.items():
                    print(f"  {k}: {v:.4f}")
                print("Validation Metrics:")
                for k, v in val_metrics.items():
                    print(f"  {k}: {v:.4f}")
                
                # Save best model
                if val_metrics['jaccard'] > best_jaccard:
                    best_jaccard = val_metrics['jaccard']
                    self.save_checkpoint('best_model.pt', epoch)
            
            # Regular checkpoint
            if epoch % 100 == 0:
                self.save_checkpoint(f'model_epoch_{epoch}.pt', epoch)

    def DTR(self):
        """Main entry point - matches your current interface"""
        self.train()